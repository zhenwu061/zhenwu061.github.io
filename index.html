<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta and Tairan He*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/k.jpg">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Zhen Wu</title>
  <meta name="Zhen Wu's Homepage" http-equiv="Content-Type" content="Zhen Wu's Homepage">
  <meta name="google-site-verification" content="o9tkXAPzuBQjAHVnDq5rkEp-uAQ9VHqusAIYu6tgkNA" />
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Zhen Wu 「吴臻」</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/wz.jpg"><img src="images/wz.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | 
    <!-- <a href="data/TairanHe_CV_20250922.pdf">CV</a> | -->
    <a href="mailto:zhenwu@stanford.edu">Email</a> |
    <!-- <a href="https://scholar.google.com/citations?user=TVWH2U8AAAAJ">Google Scholar</a> | -->
    <!-- <br/> -->
    <!-- | <a href="https://github.com/TairanHe">Github</a> |  -->
    <a href="https://www.linkedin.com/in/zhen-wu-326a70230/">LinkedIn</a> |
    </p>
    <p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 168px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=zhenkirito123&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1706734206165" data-screen-name=""></iframe><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </td>
    <td width="70%" valign="top" align="justify">
      <p>
        I am an Applied Scientist Intern at Amazon FAR (Frontier AI & Robotics), working with <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>, <a href="https://www.gshi.me/">Guanya Shi</a>, <a href="http://rockyduan.com/">Rocky Duan</a>, and <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>.
        Prior to that, I earned my Master's degree from <a href="https://www.stanford.edu/">Stanford University</a>, advised by <a href="https://tml.stanford.edu/people/karen-liu">C. Karen Liu</a>. 
        I earned my Bachelor's degree from <a href="https://english.pku.edu.cn/">Peking University</a>.
      </p>
      <p>
        My research interests are in humanoid robotics and character animation. I am particularly interested in how humans and robots perceive and interact with the world, with a focus on achieving human-level agility and dexterity.
      </p>
      <p>
        I'm always open to discussion and collaboration—feel free to drop me an email if you're interested.
      </p>
      <!-- <p>Goal: challenge conventional notions of what robots can achieve, develop robots that improves everyone's life. Focus: developing intelligent robots being able to do useful tasks with <u>intelligence, generalizability, agility and safety</u>. Method: learning-based methods that scale with the computation and data. Robots: Mobile robots, legged robots, robotic manipulators, and humanoid robots.
      </p> -->
      <!-- <p><strong>Goal:</strong> Robots that improve everyone's life.</p>
      <p><strong>Research Interest:</strong> The intersection of <u>robotics</u>, <u>large-scale machine learning</u>, and <u>general-purpose loco-manipulation</u>.</p>
      <p><strong>Research Question:</strong> How can we build <u>scalable robot learning flywheel</u> that unify <u>perception</u>, whole-body <u>control</u>, and dexterous <u>manipulation</u>, enabling reliable general-purpose robots in <u>unstructured, real-world environments</u>?</p>
      <p><strong>Robots:</strong> I love working on <u>humanoids</u> and aim to make them capable of doing everything I can do—and more.</p>
       -->
      <!-- <p><strong>Focus:</strong> How to build the <u>data flywheel for robotics</u> to unlock human-level athletic skills and semantic intelligence? How to make robots perform useful tasks with <u>adaptability, generalizability, agility, and safety</u>?</p>
      <p><strong>Method:</strong> Utilizing learning-based methods that scale with computation and data.</p> -->
      <p>Email: zhenwu [AT] stanford.edu
      </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;News</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
      <ul>
        <li>[09/2025] <a href="https://pei-xu.github.io/Basketball">Learning to Ball</a> has been accepted to SIGGRAPH Asia 2025 (Journal Track)!</li> 
        <!-- <li>[08/2025] I joined Amazon FAR (Frontier AI & Robotics) as an Applied Scientist Intern.</li>  -->
        <li>[07/2025] The code for <a href="https://hoifhli.github.io/">HIHI</a> is released in this <a href="https://github.com/zhenkirito123/hoifhli_release">repo</a>.</li> 
        <li>[06/2025] <a href="https://hoifhli.github.io/">HIHI</a> has been accepted to ICCV 2025! See you in Hawaii!</li> 
      </ul>
    </td>
  </tr>
  
</table>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://omniretarget.github.io/">
    <video autoplay loop muted src="images/omniretarget/omniretarget.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://omniretarget.github.io/" id="omniretarget_1">
      <heading>OmniRetarget: Interaction-Preserving Data Generation for Humanoid
        Whole-Body Loco-Manipulation and Scene Interaction</heading></a>
      <br>
      Lujie Yang*, Xiaoyu Huang*, <b>Zhen Wu*</b>, Angjoo Kanazawa<sup>†</sup>, Pieter Abbeel<sup>†</sup>, Carmelo Sferrazza<sup>†</sup>, C. Karen Liu<sup>†</sup>, Rocky Duan<sup>†</sup>, Guanya Shi<sup>†</sup>
      <br>
      <b>In Submission</b><br>
      </p>
  
      <div class="paper" id="omniretarget">
      <a href="https://omniretarget.github.io/">webpage</a> |
      <!-- <a href="https://arxiv.org/abs/2509.22442">pdf</a> | -->
      <a href="javascript:toggleblock('omniretarget_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('omniretarget')" class="togglebib">bibtex</a> |
      <!-- <a href="https://github.com/omniretarget/omniretarget">code</a> | -->
      <a href="https://huggingface.co/datasets/omniretarget/OmniRetarget_Dataset">dataset</a> 
  
  
      <p align="justify"> <i id="omniretarget_abs">
        A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction- preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 9-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long- horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.
      </i></p>

  <pre xml:space="preserve" style="display:none;">
@article{
}
  </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://pei-xu.github.io/Basketball">
    <video autoplay loop muted src="images/basketball/basketball.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://pei-xu.github.io/Basketball" id="basketball_1">
      <heading>Learning to Ball: Composing Policies for Long-Horizon Basketball Moves</heading></a><br>
      Pei Xu, <b>Zhen Wu</b>, Ruocheng Wang, Vishnu Sarukkai, Kayvon Fatahalian, Ioannis Karamouzas, Victor Zordan, C. Karen Liu<br>
      <b>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025)</b><br>
      </p>
  
      <div class="paper" id="basketball">
      <a href="https://pei-xu.github.io/Basketball">webpage</a> |
      <a href="https://arxiv.org/abs/2509.22442">pdf</a> |
      <a href="javascript:toggleblock('basketball_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('basketball')" class="togglebib">bibtex</a> |
      <a href="https://github.com/xupei0610/basketball">code</a> |
      <a href="https://www.bilibili.com/video/BV1BGHwzBEAd/">bilibili</a> 
  
  
      <p align="justify"> <i id="basketball_abs">
        Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases. In this paper, we introduce a novel policy integration framework to enable the composition of drastically different motor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks. We evaluate our framework on a set of fundamental basketball skills and challenging transitions. Policies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.
      </i></p>

  <pre xml:space="preserve" style="display:none;">
@article{basketball,
author = {Xu, Pei and Wu, Zhen and Wang, Ruocheng
and Sarukkai, Vishnu and Fatahalian, Kayvon
and Karamouzas, Ioannis and Zordan, Victor
and Liu, C. Karen},
title = {Learning to Ball: Composing Policies
for Long-Horizon Basketball Moves},
journal = {ACM Transactions on Graphics},
publisher = {ACM New York, NY, USA},
year = {2024},
volume = {44},
number = {6},
doi = {10.1145/3763367}
}
  </pre>
      </div>
    </td>
  </tr>

  
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://hoifhli.github.io/">
    <video autoplay loop muted src="images/hihi/hihi.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://hoifhli.github.io/" id="hihi_1">
      <heading>Human-Object Interaction from Human-Level Instructions</heading></a><br>
      <b>Zhen Wu</b>, Jiaman Li, Pei Xu, C. Karen Liu<br>
      <b>ICCV 2025</b><br>
      </p>
  
      <div class="paper" id="hihi">
      <a href="https://hoifhli.github.io/">webpage</a> |
      <a href="https://arxiv.org/abs/2406.17840">pdf</a> |
      <a href="javascript:toggleblock('hihi_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('hihi')" class="togglebib">bibtex</a> |
      <a href="https://github.com/zhenkirito123/hoifhli_release">code</a> 
  
  
      <p align="justify"> <i id="hihi_abs">
        Intelligent agents must autonomously interact with the environments to perform daily tasks based on human-level instructions. They need a foundational understanding of the world to accurately interpret these instructions, along with precise low-level movement and interaction skills to execute the derived actions. In this work, we propose the first complete system for synthesizing physically plausible, long-horizon human-object interactions for object manipulation in contextual environments, driven by human-level instructions. We leverage large language models (LLMs) to interpret the input instructions into detailed execution plans. Unlike prior work, our system is capable of generating detailed finger-object interactions, in seamless coordination with full-body movements. We also train a policy to track generated motions in physics simulation via reinforcement learning (RL) to ensure physical plausibility of the motion. Our experiments demonstrate the effectiveness of our system in synthesizing realistic interactions with diverse objects in complex environments, highlighting its potential for real-world applications.
      </i></p>

  <pre xml:space="preserve" style="display:none;">
@article{wu2024human,
title={Human-object interaction
from human-level instructions},
author={Wu, Zhen and Li, Jiaman
and Xu, Pei and Liu, C Karen},
journal={arXiv preprint arXiv:2406.17840},
year={2024}
}
  </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2503.20118">
    <video autoplay loop muted src="images/zero_hoi/zero_hoi.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2503.20118" id="zero_hoi_1">
      <heading>Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors</heading></a><br>
      Yuke Lou*, Yiming Wang*, <b>Zhen Wu</b>, Rui Zhao, Wenjia Wang, Mingyi Shi, Taku Komura<br>
      <b>In Submission</b><br>
      </p>
  
      <div class="paper" id="zero_hoi">
      <a href="https://arxiv.org/abs/2503.20118">pdf</a> |
      <a href="javascript:toggleblock('zero_hoi_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('zero_hoi')" class="togglebib">bibtex</a>
  
  
      <p align="justify"> <i id="zero_hoi_abs">
        Human-object interaction (HOI) synthesis is important for various applications, ranging from virtual reality to robotics. However, acquiring 3D HOI data is challenging due to its complexity and high cost, limiting existing methods to the narrow diversity of object types and interaction patterns in training datasets. This paper proposes a novel zero-shot HOI synthesis framework without relying on end-to-end training on currently limited 3D HOI datasets. The core idea of our method lies in leveraging extensive HOI knowledge from pre-trained Multimodal Models. Given a text description, our system first obtains temporally consistent 2D HOI image sequences using image or video generation models, which are then uplifted to 3D HOI milestones of human and object poses. We employ pre-trained human pose estimation models to extract human poses and introduce a generalizable category-level 6-DoF estimation method to obtain the object poses from 2D HOI images. Our estimation method is adaptive to various object templates obtained from text-to-3D models or online retrieval. A physics-based tracking of the 3D HOI kinematic milestone is further applied to refine both body motions and object poses, yielding more physically plausible HOI generation results. The experimental results demonstrate that our method is capable of generating open-vocabulary HOIs with physical realism and semantic diversity.
      </i></p>

  <pre xml:space="preserve" style="display:none;">
@article{lou2025zero,
title={Zero-shot human-object interaction
synthesis with multimodal priors},
author={Lou, Yuke and Wang, Yiming and Wu, Zhen
and Zhao, Rui and Wang, Wenjia and Shi, Mingyi
and Komura, Taku},
journal={arXiv preprint arXiv:2503.20118},
year={2025}
}
  </pre>
      </div>
    </td>
  </tr>

</table>


<!-- <hr> 

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Podcast</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top" align="center">
      <a href="/podcast_en">
          <img src="images/whynottv.png" alt="sym" width="60%" style="padding-top:0px; padding-bottom:0px; border-radius:35px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="/podcast_en" id="PODCAST">
      <heading>WhynotTV Podcast</heading></a><br>
      </p>

      <div class="paper" id="podcast">

        I host <a href="/podcast_en">WhynotTV Podcast</a> / <a href="/podcast_cn">WhynotTV播客</a>, a deep, professional, hardcore, long-form (2-4 hours) AI tech video podcast—focusing on 
        <br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- in-depth discussions about AI/technology;
        <br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- breaking down underlying technical details and business logic;
        <br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- while also exploring life wisdom and personal growth philosophy.
        <br>
        You can find the podcast on 
        <br>
        <a href="https://www.youtube.com/@whynottv1999">YouTube</a> | 
        <a href="https://space.bilibili.com/14145636">Bilibili</a> | 
        <a href="https://open.spotify.com/show/15liAZ2hDo4YLoyXjruq9A">Spotify</a> |
        <a href="https://podcasts.apple.com/us/podcast/whynottv-podcast/id1824936911">Apple Podcast</a> |
        <a href="https://www.xiaoyuzhoufm.com/podcast/686a1832222ae2de21fea940">小宇宙</a> | 
        <a href="https://anchor.fm/s/106f6c76c/podcast/rss">RSS</a> 
      </div>
    </td>
  </tr>
</table> -->




<!-- <hr> 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Service</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      International Conference on Robotics and Automation <b>(ICRA)</b> 2025
      <br>
      International Conference on Intelligent Robots and Systems <b>(IROS)</b> 2025
      <br>
      International Conference on Computer Vision <b>(ICCV)</b> 2025
      <br>
      IEEE Transactions on Robotics <b>(TRO)</b> 2025
      <br>
      IEEE Robotics and Automation Letters <b>(RA-L)</b> 2025
      <br>
      Robotics: Science and Systems  <b>(RSS)</b> 2025
      <br>
      International Conference on Machine Learning <b>(ICML)</b>, 2024, 2025
      <br>
      International Conference on Learning Representations <b>(ICLR)</b>, 2024, 2025
      <br>
      IEEE Conference on Decision and Control <b>(CDC)</b>, 2023
      <br>  
      Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2023, 2024
      <br>
      Learning for Dynamics & Control Conference <b>(L4DC)</b> 2023
      <br>
      AAAI Conference on Artificial Intelligence <b>(AAAI)</b> 2023, 2024, 2025
      <br>
      Conference on Robot Learning <b>(CoRL)</b> 2022, 2023, 2024
      </p>
    </td>
  </tr>
</table> -->

<hr> 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      CS224R: Deep Reinforcement Learning - Spring 2025
      <br>
      CS229: Machine Learning - Winter 2025
      <br>
      CS248B: Fundamentals of Computer Graphics: Animation and Simulation - Fall 2025
      </p>
    </td>
  </tr>
</table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br>
              <br>
              <div>
                  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=tt&d=Biz007_Pw8FVsAWycLRoKM_5XR_da9ccb8qGNbWVwnk&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
              </div>
          </td>
      </tr>
  </tbody>
</table> -->








<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="https://tairanhe.com/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('omniretarget_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('basketball_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('hihi_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('zero_hoi_abs');
</script>
</body>

</html>
